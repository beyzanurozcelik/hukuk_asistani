import os
import sys
import logging
from typing import TypedDict, List, Optional
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from langchain_ollama import ChatOllama
import os
from langchain_google_genai import ChatGoogleGenerativeAI


# --- 1. LOGGER YAPILANDIRMASI ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)],
    force=True
)
logger = logging.getLogger(__name__)

logger.info("ğŸš€ Rag Agent modÃ¼lÃ¼ yÃ¼klenmeye baÅŸladÄ±...")

# --- 2. TOOL IMPORTLARI ---
try:
    logger.info("ğŸ“¦ Search Tool'lar (point/broad) iÃ§e aktarÄ±lÄ±yor...")
    # Tools dosyanÄ±zÄ±n yeri src/tools/search_tools.py varsayÄ±lmÄ±ÅŸtÄ±r
    from src.tools.search_tools import point_search_tool, broad_search_tool
    logger.info("âœ… Tool'lar baÅŸarÄ±yla yÃ¼klendi.")
except Exception as e:
    logger.error(f"âŒ Tool'lar yÃ¼klenirken hata oluÅŸtu: {e}")
    raise

from langgraph.graph import StateGraph, END

# LLM AyarÄ±
"""llm = ChatOllama(
    model="gemma3:12b", 
    temperature=0.0, # Analiz iÃ§in 0 yaptÄ±k, kararlÄ± olsun
)
llm1 = ChatOllama(
    model="gemma3:4b-it-qat", 
    temperature=0.0, # Analiz iÃ§in 0 yaptÄ±k, kararlÄ± olsun
)"""

load_dotenv()

# --- API KEY ve MODEL ADI KONTROLÃœ ---
google_api_key = os.getenv("GOOGLE_API_KEY")
gemini_model_name = os.getenv("GEMINI_MODEL_NAME", "gemini-2.0-flash-lite") # VarsayÄ±lan: 1.5-flash

if not google_api_key:
    logger.error("ğŸš¨ GOOGLE_API_KEY bulunamadÄ±! LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin.")
    # ProgramÄ±n burada durmasÄ±nÄ± istersen:
    # sys.exit(1) 
else:
    logger.info(f"ğŸ”‘ API Key yÃ¼klendi. Hedef Model: {gemini_model_name}")

# --- 3. MODEL AYARLARI ---

# ANALÄ°Z ve CEVAP Ä°Ã‡Ä°N: .env'den gelen modeli kullanÄ±yoruz
llm1 = ChatGoogleGenerativeAI(
    model=gemini_model_name,
    temperature=0,
    max_retries=2,
    google_api_key=google_api_key 
)

# --- 3. DÄ°NAMÄ°K DOSYA LÄ°STESÄ° ALUCU ---
def get_available_files(data_path="./data"):
    """Data klasÃ¶rÃ¼ndeki PDF dosyalarÄ±nÄ±n listesini Ã§eker."""
    try:
        files = [f for f in os.listdir(data_path) if f.endswith('.pdf')]
        return files if files else ["VeritabanÄ±nda dosya bulunamadÄ±."]
    except Exception:
        return ["Veri klasÃ¶rÃ¼ okunamadÄ±."]

# --- 4. STATE VE ÅEMA ---
class RagAgentState(TypedDict):
    question: str
    decision: str
    target_source: Optional[str] # <--- YENÄ°: Hedef Dosya AdÄ±
    search_query: str
    retrieved_context: str
    response: str

class AnalysisResult(BaseModel):
    decision: str = Field(description="'Q1' (Nokta AtÄ±ÅŸÄ±) veya 'Q2' (GeniÅŸ Arama)")
    target_source: Optional[str] = Field(
        description="EÄŸer kullanÄ±cÄ± belirli bir belgeyi kastediyorsa tam dosya adÄ± (Listeden seÃ§), yoksa None",
        default=None
    )

# --- 5. DÃœÄÃœMLER (NODES) ---

def analyzer_node(state: RagAgentState):
    logger.info("ğŸ§  [ANALIZER] Soru ve Hedef Kaynak analiz ediliyor...")
    
    # Mevcut dosyalarÄ± Ã§ekip prompta gÃ¶mÃ¼yoruz
    available_files = get_available_files()
    files_str = ", ".join(available_files)
    
    structured_llm = llm1.with_structured_output(AnalysisResult)
    
    prompt = f"""Sen uzman bir Hukuk Bilgi MimarÄ± ve Arama YÃ¶neticisisin.
    
    ### MEVCUT KAYNAKLAR (DOSYALAR):
    [{files_str}]
    
    GÃ¶revin kullanÄ±cÄ± sorusunu analiz ederek 3 Ã§Ä±ktÄ± Ã¼retmektir:
    
    1. STRATEJÄ° SEÃ‡Ä°MÄ° (decision) - KRÄ°TÄ°K ADIM
    
    **Q1 (ODAKLI ARAMA - "Bul ve Getir"):**
        - Belirli bir belge ile ilgili soru soruluyorsa.
        - Bir terimin resmi tanÄ±mÄ± soruluyorsa 
        - Belirli bir sayÄ±, sÃ¼re veya limit soruluyorsa.
        - "Listele", "Say", "Nedir" gibi net olgusal talepler.
        
    **Q2 (GENÄ°Å/KEÅÄ°F ARAMA - "AraÅŸtÄ±r ve Sentezle"):**
        - SÃ¼reÃ§ ve ProsedÃ¼r sorularÄ±
        - YÃ¼kÃ¼mlÃ¼lÃ¼kler ve genel sorumluluklar
        - Senaryo ve Ã–rnek Olaylar
        - KÄ±yaslama sorularÄ± 
    2. **target_source (Hedef Kaynak):**
       - KullanÄ±cÄ± sorusunda yukarÄ±daki dosya listesinden birine atÄ±f yapÄ±yor mu? (Ã–rn: "KVKK'da", "YÃ¶netmelikte").
       - EÄER YAPIYORSA: Listeden en uygun dosya adÄ±nÄ± TAM OLARAK kopyala (Ã–rn: 'KVKK_Kanunu.pdf').
       - EÄER YAPMIYORSA veya GENEL SORUYORSA: null (None) dÃ¶ndÃ¼r.
    
    HAM SORU: {state['question']}"""
    
    result = structured_llm.invoke(prompt)
    
    source_log = result.target_source if result.target_source else "TÃœMÃœ"
    logger.info(f"âš–ï¸ KARAR: {result.decision} | KAYNAK: {source_log} | SORGU: {state['question']}")
    
    return {
        "decision": result.decision,
        "search_query": state['question'],
        "target_source": result.target_source # State'e kaydet
    }

def search_node(state: RagAgentState):
    decision = state["decision"]
    query = state["search_query"]
    target = state["target_source"] # State'den oku
    
    # Tool'lara parametreleri sÃ¶zlÃ¼k (dict) olarak geÃ§iyoruz
    tool_args = {"query": query, "target_source": target}
    
    if decision == "Q1":
        logger.info(f"ğŸ¯ [SEARCH] Nokta AtÄ±ÅŸÄ± Tetiklendi -> Kaynak: {target if target else 'None'}")
        context = point_search_tool.invoke(tool_args)
    else:
        logger.info(f"ğŸŒ [SEARCH] GeniÅŸ Arama Tetiklendi -> Kaynak: {target if target else 'None'}")
        context = broad_search_tool.invoke(tool_args)
        
    logger.info(f"ğŸ“š [SEARCH] Veri Ã§ekildi (Uzunluk: {len(context)} karakter)")
    return {"retrieved_context": context}

def quality_control_node(state: RagAgentState):
    # Ä°leride buraya "Context boÅŸsa tekrar ara" mantÄ±ÄŸÄ± eklenebilir
    return state

def responder_node(state: RagAgentState):
    logger.info("âœï¸ [RESPONDER] Cevap yazÄ±lÄ±yor...")
    
    # Prompt'a hangi kaynaÄŸa bakÄ±ldÄ±ÄŸÄ±nÄ± da ekleyelim ki LLM bilsin
    source_info = f"OdaklanÄ±lan Kaynak: {state['target_source']}" if state['target_source'] else "Kaynak: TÃ¼m VeritabanÄ±"
    
    prompt = f"""Sen profesyonel bir hukuk asistanÄ±sÄ±n. AÅŸaÄŸÄ±daki baÄŸlamÄ± kullanarak soruyu cevapla.

    BAÄLAM:
    {state['retrieved_context']}
    
    SORU: {state['question']}
    
    CevabÄ± hukuki dille, maddelere atÄ±f yaparak ve net bir ÅŸekilde ver. CevabÄ±nda kaynaklarÄ± belirt."""
    
    res = llm1.invoke(prompt)
    logger.info("âœ… [RESPONDER] Ä°ÅŸlem tamam.")
    return {"response": res.content}

# --- 6. WORKFLOW KURULUMU ---

def create_rag_agent():
    workflow = StateGraph(RagAgentState)

    workflow.add_node("analizer", analyzer_node)
    workflow.add_node("search", search_node)
    workflow.add_node("quality_control", quality_control_node)
    workflow.add_node("responder", responder_node)

    workflow.set_entry_point("analizer")
    workflow.add_edge("analizer", "search")
    workflow.add_edge("search", "quality_control")
    workflow.add_edge("quality_control", "responder")
    workflow.add_edge("responder", END)

    return workflow.compile()

rag_agent = create_rag_agent()

# --- 7. TEST ---
if __name__ == "__main__":
    # Test Senaryosu 1: Kaynak BelirtilmiÅŸ
    print("\n--- TEST 1: KaynaklÄ± Sorgu ---")
    try:
        # Ã–rnek: Data klasÃ¶rÃ¼nde 'KVKK_Kanunu.pdf' olduÄŸunu varsayÄ±yoruz
        q1 = "KVKK metninde madde 5 ne diyor?" 
        final_state = rag_agent.invoke({"question": q1})
        print(f"CEVAP: {final_state['response'][:200]}...") 
    except Exception as e:
        print(f"Hata: {e}")

    # Test Senaryosu 2: Genel Sorgu
    print("\n--- TEST 2: Genel Sorgu ---")
    try:
        q2 = "Veri sorumlusunun yÃ¼kÃ¼mlÃ¼lÃ¼kleri nelerdir?"
        final_state = rag_agent.invoke({"question": q2})
        print(f"CEVAP: {final_state['response'][:200]}...")
    except Exception as e:
        print(f"Hata: {e}")