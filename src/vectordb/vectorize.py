import os
import shutil
import logging
import sys
import stat
import time
import re
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
from src.vectordb.embedding import get_embedding_model
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import pickle
from langchain_community.retrievers import BM25Retriever

load_dotenv()

# Logger Ayarlarƒ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)],
    force=True
)
logger = logging.getLogger(__name__)

PERSIST_DIR = "./chromadb"

def get_chroma_client():
    embedding_function = get_embedding_model()
    return Chroma(
        persist_directory=PERSIST_DIR,
        embedding_function=embedding_function,
        collection_name="legal_rag_collection"
    )

def remove_readonly(func, path, excinfo):
    os.chmod(path, stat.S_IWRITE)
    func(path)

def clear_database():
    if os.path.exists(PERSIST_DIR):
        try:
            shutil.rmtree(PERSIST_DIR, onexc=remove_readonly)
            logger.warning(f"‚ö†Ô∏è Veritabanƒ± temizlendi.")
            time.sleep(1)
        except Exception:
            pass

def regex_madde_split(full_text, source_name):
    """
    Metni 'MADDE X' ibarelerine g√∂re b√∂ler.
    """
    logger.info(f"‚úÇÔ∏è Regex ile Madde Madde b√∂l√ºn√ºyor... ({len(full_text)} karakter)")
    
    # --- REGEX DESENƒ∞ ---
    # (?=...) : Lookahead. Yani "MADDE" kelimesini g√∂rd√ºƒü√ºn yerden b√∂l ama kelimeyi silme.
    # \n : Yeni satƒ±r ba≈üƒ±ndaki maddeleri arar (C√ºmle i√ßindekileri almaz).
    # MADDE\s+\d+ : "MADDE" + Bo≈üluk + Sayƒ± (√ñrn: MADDE 1, MADDE 14)
    pattern = r"(?=\nMADDE\s+\d+)"
    
    chunks = re.split(pattern, full_text)
    
    # Eƒüer hi√ß madde bulamazsa (√ñrn: Giri≈ü kƒ±smƒ±, √ñns√∂z veya Madde i√ßermeyen belge)
    if len(chunks) < 2:
        logger.warning("‚ö†Ô∏è Metinde 'MADDE' yapƒ±sƒ± bulunamadƒ±. Standart paragraf b√∂lmeye ge√ßiliyor.")
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, 
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", " "]
        )
        return splitter.create_documents([full_text], metadatas=[{"source": source_name, "split_method": "recursive"}])

    final_docs = []
    
    for chunk in chunks:
        clean_chunk = chunk.strip()
        
        # √áok kƒ±sa par√ßalarƒ± (sayfa no, √ß√∂p karakter) atla
        if len(clean_chunk) < 20:
            continue
            
        # --- METADATA ZEKASI ---
        # Chunk'ƒ±n hangi madde olduƒüunu bulup veritabanƒ±na etiket olarak ekleyelim.
        # Bu, ileride "Bana sadece Madde 5'i getir" dediƒüinde hayat kurtarƒ±r.
        madde_match = re.search(r"(MADDE\s+\d+)", clean_chunk)
        madde_tag = madde_match.group(1) if madde_match else "Giri≈ü/Diƒüer"
        
        enriched_content = f" {source_name} |{madde_tag} \n---\n{clean_chunk}"
        
        final_docs.append(Document(
            page_content=enriched_content, # <--- Vekt√∂r artƒ±k bunu kullanacak!
            metadata={
                "source": source_name,
                "madde_no": madde_tag,
                "split_method": "regex_madde"
            }
        ))
        
    logger.info(f"‚úÖ Ba≈üarƒ±lƒ±: {len(final_docs)} adet madde tespit edildi.")
    return final_docs

def process_and_save_pdfs(reset_db=False):
    if reset_db:
        clear_database()
        
    data_path = "./data"
    all_final_documents = []

    if not os.path.exists(data_path):
        logger.error("Data klas√∂r√º yok!")
        return

    pdf_files = [f for f in os.listdir(data_path) if f.endswith('.pdf')]
    
    for pdf_file in pdf_files:
        file_path = os.path.join(data_path, pdf_file)
        logger.info(f"üìÇ Dosya Y√ºkleniyor: {pdf_file}")
        
        try:
            loader = PyPDFLoader(file_path)
            pages = loader.load()
            
            # √ñNEMLƒ∞: Regex'in sayfa ge√ßi≈ülerinde √ßalƒ±≈üabilmesi i√ßin
            # t√ºm sayfalarƒ± tek bir dev metin (string) haline getiriyoruz.
            full_text = "\n".join([p.page_content for p in pages])
            
            # Regex Splitter'ƒ± √ßaƒüƒ±r
            file_docs = regex_madde_split(full_text, source_name=pdf_file)
            
            all_final_documents.extend(file_docs)
                
        except Exception as e:
            logger.error(f"{pdf_file} hata: {e}")

    if all_final_documents:
        db = get_chroma_client()
        
        # Batch Processing (Veri tabanƒ± ≈üi≈ümesin diye 100'erli ekliyoruz)
        batch_limit = 100
        for i in range(0, len(all_final_documents), batch_limit):
            batch = all_final_documents[i : i + batch_limit]
            db.add_documents(batch)
            logger.info(f"üíæ {len(batch)} kayƒ±t veritabanƒ±na yazƒ±ldƒ±...")
            
        logger.info(f"‚úÖ T√úM ƒ∞≈ûLEM TAMAM: Toplam {len(all_final_documents)} chunk hazƒ±r.")
    else:
        logger.warning("Veri yok.")
    
    if all_final_documents:
        # 1. ChromaDB (Vekt√∂rler) kaydediliyor
        db = get_chroma_client()
        db.add_documents(all_final_documents)
        
        # 2. BM25 Objesini Olu≈ütur ve "Pi≈üir"
        logger.info("üç≥ BM25 indeksi hesaplanƒ±yor ve donduruluyor...")
        bm25_retriever = BM25Retriever.from_documents(all_final_documents)
        
        # 3. Hazƒ±r objeyi diske yaz
        with open("./chromadb/bm25_retriever.pkl", "wb") as f:
            pickle.dump(bm25_retriever, f)
            
        logger.info("‚úÖ BM25 'hazƒ±r paket' olarak kaydedildi.")

if __name__ == "__main__":
    process_and_save_pdfs(reset_db=True)